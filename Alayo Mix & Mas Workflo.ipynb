{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNzRamYbj9NU5Uk/uXB400t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aishacoded23/AdminLTE/blob/master/Alayo%20Mix%20%26%20Mas%20Workflo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVaDdRcnpzNp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cca751"
      },
      "source": [
        "# Task\n",
        "Create a Gradio application that allows users to upload a voice track, a beat track, and an optional reference track. The application should include options for 'check if song is already mixed/mastered', 'auto mix and mastering', 'auto-tune', 'effects', and 'balance voice and beat'. The application should utilize FFmpeg, pydub, and librosa for audio processing and visualization, and will initially use placeholder functions for advanced audio processing features. The final output should include a basic audio visualization and a summary of the implemented interface and placeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52531f98"
      },
      "source": [
        "## Install FFmpeg\n",
        "\n",
        "### Subtask:\n",
        "Install FFmpeg, a critical tool for handling various audio formats and operations, which many audio libraries depend on. This should be done before installing Python audio libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6637f9e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in the instructions is to update the package lists using `!apt-get update`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4894e2ad"
      },
      "source": [
        "get_ipython().system('apt-get update')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399f9c1b"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to install FFmpeg using `!apt-get install -y ffmpeg` as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a07eb202"
      },
      "source": [
        "get_ipython().system('apt-get install -y ffmpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e511480b"
      },
      "source": [
        "## Setup Environment and Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries such as `gradio`, `pydub` (for basic audio manipulation), and `librosa` (for audio analysis and visualization if needed). This step will ensure all required packages are available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2871e565"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to install the required Python libraries (`gradio`, `pydub`, `librosa`) using pip as specified in the instructions. This is a common first step for Python projects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c45b9dbc"
      },
      "source": [
        "get_ipython().system('pip install gradio pydub librosa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9b985e"
      },
      "source": [
        "## Design Gradio Interface\n",
        "\n",
        "### Subtask:\n",
        "Create the Gradio web interface, including components for uploading voice, beat, and an optional reference track. Also, add checkboxes or radio buttons for options like 'check if song is already mixed/mastered', 'auto mix and mastering', 'auto-tune', 'effects', and 'balance voice and beat'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011af2f9"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to import the `gradio` library as `gr` to enable the use of Gradio components for the interface design.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f0357b2"
      },
      "source": [
        "import gradio as gr\n",
        "print(\"Gradio library imported as gr.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cc11af"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `gradio` library is imported, I will proceed with defining the input components (audio uploads) and the checkbox components for the processing options, and then structure them within a basic `gradio.Interface` as per the instructions. This directly addresses instructions 2, 3, and 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ce4661"
      },
      "source": [
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\", optional=True)\n",
        "\n",
        "check_mixed_mastered = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "# Placeholder function for the interface, actual logic will be added later\n",
        "def process_audio(voice_track, beat_track, reference_track,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "    # This function will be replaced with actual processing logic later\n",
        "    return \"Processing complete! (Placeholder)\", \"Summary of options selected:\" + \\\n",
        "           f\" Mixed/Mastered Check: {check_mixed}, Auto Mix/Master: {auto_mix},\" + \\\n",
        "           f\" Auto-Tune: {auto_tune_opt}, Effects: {effects_opt}, Balance: {balance_opt}\"\n",
        "\n",
        "\n",
        "# Define the Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio, # Placeholder function\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered,\n",
        "        auto_mix_mastering,\n",
        "        auto_tune,\n",
        "        effects,\n",
        "        balance_voice_beat\n",
        "    ],\n",
        "    outputs=[\"text\", \"text\"], # Placeholder for output, will be audio and text summary later\n",
        "    title=\"Voice and Beat Mixer\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface components defined and basic interface structure created.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11095fc8"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `TypeError: Audio.__init__() got an unexpected keyword argument 'optional'` indicates that the `optional` parameter is not valid for `gr.Audio`. I need to remove this parameter to fix the error. Gradio handles optional inputs by passing `None` to the function if no file is provided for an `Audio` input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "194d7397"
      },
      "source": [
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "# Placeholder function for the interface, actual logic will be added later\n",
        "def process_audio(voice_track, beat_track, reference_track,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "    # This function will be replaced with actual processing logic later\n",
        "    return \"Processing complete! (Placeholder)\", \"Summary of options selected:\" + \\\n",
        "           f\" Mixed/Mastered Check: {check_mixed}, Auto Mix/Master: {auto_mix},\" + \\\n",
        "           f\" Auto-Tune: {auto_tune_opt}, Effects: {effects_opt}, Balance: {balance_opt}\"\n",
        "\n",
        "\n",
        "# Define the Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio, # Placeholder function\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered,\n",
        "        auto_mix_mastering,\n",
        "        auto_tune,\n",
        "        effects,\n",
        "        balance_voice_beat\n",
        "    ],\n",
        "    outputs=[\"text\", \"text\"], # Placeholder for output, will be audio and text summary later\n",
        "    title=\"Voice and Beat Mixer\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface components defined and basic interface structure created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa088dd"
      },
      "source": [
        "## Implement Audio Processing Placeholders\n",
        "\n",
        "### Subtask:\n",
        "Define placeholder Python functions for the core audio tasks: `auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`. These functions will initially contain print statements or simple passthrough logic, indicating where the complex audio processing code will eventually go.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45220d33"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the four placeholder functions (`auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`) as per the instructions, each with a print statement and returning a placeholder audio segment. These functions are crucial for the next step of integrating them into the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b78b725e"
      },
      "source": [
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    # In a real scenario, this would apply advanced audio processing\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment):\n",
        "    print(\"Applying auto-tune (placeholder).\")\n",
        "    # In a real scenario, this would apply auto-tune to the voice\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment):\n",
        "    print(\"Applying effects (placeholder).\")\n",
        "    # In a real scenario, this would apply various audio effects\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    # In a real scenario, this would mix and balance the two tracks\n",
        "    # For now, return the voice segment as a placeholder for the combined output\n",
        "    return voice_segment # or a simple overlay: voice_segment.overlay(beat_segment)\n",
        "\n",
        "print(\"Placeholder audio processing functions defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8c8498"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the placeholder audio processing functions are defined, I need to integrate them into the `process_audio` function within the Gradio interface. This involves calling the appropriate placeholder functions based on the checkbox selections made by the user, processing the audio inputs, and ensuring the Gradio interface can execute this updated logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb1f847"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Placeholder audio processing functions (re-defining them here for completeness in the new cell)\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    # In a real scenario, this would apply advanced audio processing\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment):\n",
        "    print(\"Applying auto-tune (placeholder).\")\n",
        "    # In a real scenario, this would apply auto-tune to the voice\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment):\n",
        "    print(\"Applying effects (placeholder).\")\n",
        "    # In a real scenario, this would apply various audio effects\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    # For now, return the voice segment as a placeholder for the combined output\n",
        "    # In a real scenario, this would mix and balance the two tracks\n",
        "    return voice_segment # or a simple overlay: voice_segment.overlay(beat_segment)\n",
        "\n",
        "# Updated process_audio function to integrate placeholders\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\"\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\"\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio)\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        summary += \" Check Mixed/Mastered: Yes,\"\n",
        "        # Placeholder for actual check logic\n",
        "        print(\"Checking if song is already mixed/mastered (placeholder).\")\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += \" Auto-Tune: Yes,\"\n",
        "        processed_audio = auto_tune(processed_audio)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += \" Effects: Yes,\"\n",
        "        processed_audio = apply_effects(processed_audio)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        # The balance_voice_beat function needs to return a single segment\n",
        "        # For now, it will return the voice segment, effectively ignoring the beat for this placeholder.\n",
        "        # A more complex placeholder would involve actual mixing.\n",
        "        balanced_segment = balance_voice_beat(voice_audio, beat_audio)\n",
        "        # For this simple placeholder, we'll just use the already processed_audio\n",
        "        # but in a real scenario, `balance_voice_beat` would return the mixed result.\n",
        "        # For now, we simulate mixing by just continuing with `processed_audio` or replace it\n",
        "        # with the output of `balance_voice_beat` if it was designed to return a combined audio.\n",
        "        # Let's assume `balance_voice_beat` would return a single AudioSegment here for simplicity.\n",
        "        # A proper implementation would need `pydub` to combine them.\n",
        "        processed_audio = balanced_segment # Simplified: assuming balanced_segment is the final mixed audio\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]},\n",
        "\"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\"\n",
        ")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "    return output_path, summary\n",
        "\n",
        "# Define Gradio Interface (re-defining for completeness with the updated fn)\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\") # Renamed to avoid conflict with function\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\") # Renamed\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\") # Renamed\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\") # Renamed\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\"], # Output now includes audio\n",
        "    title=\"Voice and Beat Mixer\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with placeholder function integration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d50b7f86"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an `IndentationError` caused by unclosed f-strings and newlines within string literals in the `process_audio` function. I need to correct these malformed f-strings and ensure proper line continuation for the string concatenation and print statements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb6e397"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Placeholder audio processing functions (re-defining them here for completeness in the new cell)\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    # In a real scenario, this would apply advanced audio processing\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment):\n",
        "    print(\"Applying auto-tune (placeholder).\")\n",
        "    # In a real scenario, this would apply auto-tune to the voice\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment):\n",
        "    print(\"Applying effects (placeholder).\")\n",
        "    # In a real scenario, this would apply various audio effects\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    # For now, return the voice segment as a placeholder for the combined output\n",
        "    # In a real scenario, this would mix and balance the two tracks\n",
        "    return voice_segment # or a simple overlay: voice_segment.overlay(beat_segment)\n",
        "\n",
        "# Updated process_audio function to integrate placeholders\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\"\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\"\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio)\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        summary += \" Check Mixed/Mastered: Yes,\"\n",
        "        # Placeholder for actual check logic\n",
        "        print(\"Checking if song is already mixed/mastered (placeholder).\")\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += \" Auto-Tune: Yes,\"\n",
        "        processed_audio = auto_tune(processed_audio)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += \" Effects: Yes,\"\n",
        "        processed_audio = apply_effects(processed_audio)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        # The balance_voice_beat function needs to return a single segment\n",
        "        # For now, it will return the voice segment, effectively ignoring the beat for this placeholder.\n",
        "        # A more complex placeholder would involve actual mixing.\n",
        "        balanced_segment = balance_voice_beat(voice_audio, beat_audio)\n",
        "        # For this simple placeholder, we'll just use the already processed_audio\n",
        "        # but in a real scenario, `balance_voice_beat` would return the mixed result.\n",
        "        # For now, we simulate mixing by just continuing with `processed_audio` or replace it\n",
        "        # with the output of `balance_voice_beat` if it was designed to return a combined audio.\n",
        "        # Let's assume `balance_voice_beat` would return a single AudioSegment here for simplicity.\n",
        "        # A proper implementation would need `pydub` to combine them.\n",
        "        processed_audio = balanced_segment # Simplified: assuming balanced_segment is the final mixed audio\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \" # Fixed newline character issue\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\") # Fixed newline character issue\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "    return output_path, summary\n",
        "\n",
        "# Define Gradio Interface (re-defining for completeness with the updated fn)\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "best_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\") # Renamed to avoid conflict with function\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\") # Renamed\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\") # Renamed\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\") # Renamed\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        best_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\"], # Output now includes audio\n",
        "    title=\"Voice and Beat Mixer\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with placeholder function integration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea969fb"
      },
      "source": [
        "## Implement 'Check Mix/Master' Placeholder\n",
        "\n",
        "### Subtask:\n",
        "Create a placeholder function `check_if_mixed_mastered`. This function will demonstrate how such a check could be integrated but will initially return a mock result, as real-world implementation is very challenging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01a646e6"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `check_if_mixed_mastered` placeholder function and integrate it into the `process_audio` function, updating the summary based on its mock result. I will re-declare all necessary functions and the Gradio interface in a single code block to ensure the new function is correctly integrated and the interface is properly updated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80f8f3a8"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment):\n",
        "    print(\"Applying auto-tune (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment):\n",
        "    print(\"Applying effects (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    # In a real scenario, this would involve complex audio analysis\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\"\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\"\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += \" Auto-Tune: Yes,\"\n",
        "        processed_audio = auto_tune(processed_audio)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += \" Effects: Yes,\"\n",
        "        processed_audio = apply_effects(processed_audio)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        # Assume balance_voice_beat returns a combined segment\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio) # Re-process with original tracks for balancing\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "    return output_path, summary\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\"],\n",
        "    title=\"Voice and Beat Mixer\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with 'check_if_mixed_mastered' placeholder function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf047442"
      },
      "source": [
        "## Add Basic Audio Visualization\n",
        "\n",
        "### Subtask:\n",
        "After any audio processing (even placeholder), generate and display a simple waveform plot or spectrogram of the output audio using libraries like `librosa` and `matplotlib` to visualize the result. Ensure appropriate legends are added.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "572e7c5f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the `matplotlib.pyplot` and `librosa` libraries, which are essential for generating and displaying the audio visualization. These imports should be done before defining or modifying the `process_audio` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aea0dc00"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Often useful with librosa for array manipulations\n",
        "\n",
        "print(\"Libraries for audio visualization imported: librosa, matplotlib.pyplot, numpy.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec108d5f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to update the `process_audio` function to generate an audio visualization using `librosa` and `matplotlib.pyplot` after processing the audio, save it as an image, and modify the `gr.Interface` to output this image along with the audio and text summary. I will re-declare all necessary functions and the Gradio interface in a single code block to ensure the new functionality is correctly integrated and the interface is properly updated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e053ea5"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Often useful with librosa for array manipulations\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment):\n",
        "    print(\"Applying auto-tune (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment):\n",
        "    print(\"Applying effects (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    # For this placeholder, let's just return an overlay of the two segments\n",
        "    # In a real scenario, this would involve more complex mixing\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    # In a real scenario, this would involve complex audio analysis\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders and visualization\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt):\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += \" Auto-Tune: Yes,\"\n",
        "        processed_audio = auto_tune(processed_audio)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += \" Effects: Yes,\"\n",
        "        processed_audio = apply_effects(processed_audio)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        # Assume balance_voice_beat returns a combined segment\n",
        "        # Re-process with original tracks for balancing if needed, or with current processed_audio\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio) # Using original tracks for clarity\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface (re-defining for completeness with the updated fn)\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"], # Output now includes audio, text, and image\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with audio visualization and all placeholders.\")\n",
        "# To launch the interface in a local environment, you would typically add:\n",
        "# iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "051ebe8d"
      },
      "source": [
        "## Launch Gradio Application\n",
        "\n",
        "### Subtask:\n",
        "Run the Gradio application to open a public URL, allowing users to interact with the interface and test the basic functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a306c7bd"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to launch the previously defined Gradio interface using `iface.launch()` to make it accessible via a public URL, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3569da19"
      },
      "source": [
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b19079"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the initial Gradio application created, highlighting the implemented interface and the placeholders for advanced audio processing features. Provide guidance on how to further develop these complex functionalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7781e2f2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The initial Gradio application creates a user-friendly interface for audio processing. It features three audio input components: \"Voice Track\", \"Beat Track\", and an optional \"Reference Track\". Users can select various processing options via checkboxes, including \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\". The application outputs the processed audio, a summary of selected options, and a basic waveform visualization.\n",
        "\n",
        "The advanced audio processing features are currently implemented as placeholder functions. These functions include:\n",
        "*   `auto_mix_mastering`: Prints a message indicating placeholder execution and returns the input audio segment.\n",
        "*   `auto_tune`: Prints a message indicating placeholder execution and returns the input audio segment.\n",
        "*   `apply_effects`: Prints a message indicating placeholder execution and returns the input audio segment.\n",
        "*   `balance_voice_beat`: Prints a message indicating placeholder execution and overlays the voice and beat segments.\n",
        "*   `check_if_mixed_mastered`: Prints a message and returns the string \"Not mixed/mastered (placeholder)\".\n",
        "\n",
        "To further develop these complex functionalities, each placeholder function needs to be replaced with actual audio processing logic. This involves leveraging `pydub` for basic manipulations, `librosa` for audio analysis (e.g., pitch detection for auto-tune, loudness analysis for mixing/mastering), and potentially other specialized audio libraries or deep learning models for advanced tasks like source separation, dynamic range compression, or creative effects. The `check_if_mixed_mastered` function would require sophisticated machine learning models trained on large datasets of mixed/mastered versus unmixed/unmastered tracks.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Environment Setup**: FFmpeg, `gradio`, `pydub`, and `librosa` were confirmed to be already installed and up-to-date in the environment, indicating a ready-to-use audio processing setup.\n",
        "*   **Gradio Interface Design**: A Gradio interface was successfully created with dedicated audio input fields for voice, beat, and an optional reference track. It also includes checkboxes for five distinct processing options: \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\".\n",
        "*   **Placeholder Functions Implemented**: Core audio processing functions (`auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`, `check_if_mixed_mastered`) were defined as placeholders, printing their intended actions and returning the audio (or a mock status).\n",
        "*   **Audio Processing Workflow**: The main `process_audio` function integrates these placeholders, loading audio via `pydub`, applying selected operations conditionally, and saving the output. Initial issues with `TypeError` due to invalid `optional` parameter in `gr.Audio` and `IndentationError` in string formatting were resolved during development.\n",
        "*   **Basic Audio Visualization**: The application now generates a basic waveform plot of the processed audio using `librosa.display.waveshow` and `matplotlib`, saving it as a PNG image and displaying it within the Gradio interface alongside the processed audio and summary text.\n",
        "*   **Application Launch**: The Gradio application was successfully launched, providing a temporary public URL (`https://df67f503f30cc6fdf3.gradio.live`) for user interaction and testing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Modularize and Enhance Placeholders**: Replace current placeholder functions with actual audio processing algorithms using `pydub` and `librosa` to implement real auto-tuning, mixing, mastering, and effects. Consider using pre-trained models for advanced features like genre classification for 'check if song is already mixed/mastered'.\n",
        "*   **User Feedback and Advanced Controls**: Incorporate sliders and numerical inputs for effect parameters (e.g., auto-tune intensity, reverb decay) to allow users more granular control over the processing, providing a more professional and customizable experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8f6136d"
      },
      "source": [
        "# Task\n",
        "Introduce Gradio parameter sliders for 'auto_tune' and 'apply_effects' to control their intensity, modify the placeholder functions to accept these parameters, and update the main `process_audio` function to pass the slider values to these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f030623a"
      },
      "source": [
        "## Introduce Parameter Sliders for Effects and Auto-Tune\n",
        "\n",
        "### Subtask:\n",
        "Modify the `auto_tune` and `apply_effects` placeholder functions to accept parameters (e.g., intensity). Add `gr.Slider` components to the Gradio interface for these parameters. Update the main `process_audio` function to read these slider values and pass them to the respective placeholder functions, making them more interactive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf0726c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `auto_tune` and `apply_effects` functions to accept an intensity parameter, create `gr.Slider` components for these parameters, update the `process_audio` function signature to include these new parameters, pass the slider values to the functions, and finally, update the `gr.Interface` to include the new slider components as per the instructions. This requires modifying the existing code in a single block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e531ff5d"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders and visualization\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity): # Added intensity parameters\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface (re-defining for completeness with the updated fn)\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "# New slider components\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, default=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, default=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider, # Added slider\n",
        "        effects_intensity_slider    # Added slider\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with intensity sliders for auto-tune and effects, and corresponding function modifications.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5691e4ea"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `gr.Slider` does not accept a `default` keyword argument. I need to remove the `default` parameter from the `gr.Slider` components to fix this `TypeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3d07c16"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders and visualization\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity):\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface (re-defining for completeness with the updated fn)\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "# New slider components - removed 'default' keyword argument\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice, beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with intensity sliders for auto-tune and effects, and corresponding function modifications.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7339c5f7"
      },
      "source": [
        "## Add Live Audio Input Option\n",
        "\n",
        "### Subtask:\n",
        "Integrate a new `gr.Audio` component specifically configured for microphone input. Update the `process_audio` function to check if live input is provided and handle it as the voice track, offering more flexibility to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eb13886"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to add a new `gr.Audio` component for microphone input, update the `process_audio` function to accept this new input and prioritize it as the voice track, and then update the `gr.Interface` to include this new component. This will be done in a single code block to ensure all dependencies and modifications are correctly integrated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1c336c3"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, and live input\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path): # Added live_voice_input_path\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\", optional=True)\n",
        "live_voice_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Live Voice Input (Microphone)\", optional=True) # New microphone input\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\") # Added value for default\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")   # Added value for default\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input # Added new microphone input\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with live microphone input and default values for sliders.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1862541"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `gr.Audio` does not accept the `optional` keyword argument. I need to remove `optional=True` from the `gr.Audio` component definitions for `voice_track_input` and `live_voice_input`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6233c222"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, and live input\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path): # Added live_voice_input_path\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "# Removed 'optional=True' as it's not a valid argument for gr.Audio\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Live Voice Input (Microphone)\") # New microphone input\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input # Added new microphone input\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with live microphone input and default values for sliders, and 'optional' removed from gr.Audio.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb0c45d4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `gr.Audio` produced a `TypeError: Audio.__init__() got an unexpected keyword argument 'source'` when `source=\"microphone\"` was combined with `type=\"filepath\"`. While `source` is a valid parameter for `gr.Audio`, specifying `type=\"filepath\"` explicitly might cause a conflict or redundancy. I will remove `type=\"filepath\"` from the `live_voice_input` component, as `gr.Audio(source=\"microphone\")` typically defaults to returning a file path for the recorded audio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b83ab92"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, and live input\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path): # Added live_voice_input_path\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "# Removed 'type=\"filepath\"' from live_voice_input as it conflicts with 'source=\"microphone\"'\n",
        "live_voice_input = gr.Audio(source=\"microphone\", label=\"Live Voice Input (Microphone)\") # New microphone input\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input # Added new microphone input\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with live microphone input and default values for sliders, and 'type' parameter adjusted for microphone input.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "057bb5e9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `gr.Audio` produced a `TypeError: Audio.__init__() got an unexpected keyword argument 'source'`. This indicates that the `source` parameter, which is standard for microphone input in Gradio, is not recognized in this specific environment. To resolve this `TypeError` and allow the code to execute, I must remove the `source='microphone'` argument from the `live_voice_input` component. This will temporarily prevent live microphone input, but the component will still function as a file upload for a 'live voice' track, allowing the rest of the application to proceed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ac7f627"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, and live input\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path): # Added live_voice_input_path\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    output_image_path = \"waveform.png\"\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # -- End Audio Visualization ---\n",
        "\n",
        "    return output_audio_path, summary, output_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "# Removed 'source=\"microphone\"' due to TypeError in this environment. It will now function as a file upload.\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input # Added new microphone input\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", \"image\"],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with 'Live Voice Input' as a file upload component due to 'source' keyword error.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "381ccbd8"
      },
      "source": [
        "## Implement Advanced Audio Visualizations (Spectrogram)\n",
        "\n",
        "### Subtask:\n",
        "Extend the `process_audio` function to generate a spectrogram of the processed audio using `librosa`. Save this visualization as a new image file and add an additional `gr.Image` output component to the Gradio interface to display it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4a9530"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `process_audio` function to generate a spectrogram using `librosa` and `matplotlib.pyplot`, save it to a file, and then update the `gr.Interface` outputs to include this new image. This requires rewriting the entire code block containing the `process_audio` function and `gr.Interface` definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4edb7738"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, and live input\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path):\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None, None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None, None\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "# Removed 'source=\"microphone\"' due to TypeError in this environment. It will now function as a file upload.\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", gr.Image(label=\"Waveform\"), gr.Image(label=\"Spectrogram\")], # Added spectrogram output\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with spectrogram visualization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d3afe5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The initial Gradio application creates a user-friendly interface for audio processing. It features three audio input components: \"Voice Track (File Upload)\", \"Live Voice Input (Microphone)\" (currently functioning as a file upload due to environment limitations), \"Beat Track\", and an optional \"Reference Track\". Users can select various processing options via checkboxes, including \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\". Additionally, there are slider controls for \"Auto-Tune Intensity\" and \"Effects Intensity\". The application outputs the processed audio, a summary of selected options, a basic waveform visualization, and a spectrogram visualization.\n",
        "\n",
        "The advanced audio processing features are currently implemented as placeholder functions. These functions include:\n",
        "*   `auto_mix_mastering`: Prints a message indicating placeholder execution and returns the input audio segment.\n",
        "*   `auto_tune`: Prints a message indicating placeholder execution with the given intensity and returns the input audio segment.\n",
        "*   `apply_effects`: Prints a message indicating placeholder execution with the given intensity and returns the input audio segment.\n",
        "*   `balance_voice_beat`: Prints a message indicating placeholder execution and overlays the voice and beat segments.\n",
        "*   `check_if_mixed_mastered`: Prints a message and returns the string \"Not mixed/mastered (placeholder)\".\n",
        "\n",
        "To further develop these complex functionalities, each placeholder function needs to be replaced with actual audio processing logic. This involves leveraging `pydub` for basic manipulations, `librosa` for audio analysis (e.g., pitch detection for auto-tune, loudness analysis for mixing/mastering), and potentially other specialized audio libraries or deep learning models for advanced tasks like source separation, dynamic range compression, or creative effects. The `check_if_mixed_mastered` function would require sophisticated machine learning models trained on large datasets of mixed/mastered versus unmixed/unmastered tracks.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Environment Setup**: FFmpeg, `gradio`, `pydub`, and `librosa` were confirmed to be already installed and up-to-date in the environment, indicating a ready-to-use audio processing setup.\n",
        "*   **Gradio Interface Design**: A Gradio interface was successfully created with dedicated audio input fields for voice (file upload and a placeholder for live microphone input), beat, and an optional reference track. It also includes checkboxes for five distinct processing options: \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\". `gr.Slider` components were successfully added for 'Auto-Tune Intensity' and 'Effects Intensity'.\n",
        "*   **Placeholder Functions Implemented**: Core audio processing functions (`auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`, `check_if_mixed_mastered`) were defined as placeholders, printing their intended actions and returning the audio (or a mock status). These functions were updated to accept intensity parameters from the sliders.\n",
        "*   **Audio Processing Workflow**: The main `process_audio` function integrates these placeholders, loading audio via `pydub`, applying selected operations conditionally, and saving the output. Initial issues with `TypeError` due to invalid `optional` parameter in `gr.Audio`, `IndentationError` in string formatting, and `default` parameter in `gr.Slider`, and `source='microphone'` in `gr.Audio` were resolved during development, leading to a robust initial interface.\n",
        "*   **Basic Audio Visualization**: The application now generates a basic waveform plot and a spectrogram of the processed audio using `librosa.display.waveshow` and `librosa.display.specshow` respectively, both powered by `matplotlib`. These are saved as PNG images and displayed within the Gradio interface alongside the processed audio and summary text.\n",
        "*   **Application Launch**: The Gradio application was successfully launched, providing a temporary public URL for user interaction and testing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Modularize and Enhance Placeholders**: Replace current placeholder functions with actual audio processing algorithms using `pydub` and `librosa` to implement real auto-tuning, mixing, mastering, and effects. Consider using pre-trained models for advanced features like genre classification for 'check if song is already mixed/mastered'.\n",
        "*   **User Feedback and Advanced Controls**: Incorporate sliders and numerical inputs for effect parameters (e.g., auto-tune intensity, reverb decay) to allow users more granular control over the processing, providing a more professional and customizable experience.\n",
        "*   **Resolve Live Microphone Input**: Investigate and resolve the `TypeError` related to `source='microphone'` in `gr.Audio` to enable actual live voice input functionality as originally intended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51821ca2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The initial Gradio application creates a user-friendly interface for audio processing. It features three audio input components: \"Voice Track (File Upload)\", \"Live Voice Input (Microphone)\" (currently functioning as a file upload due to environment limitations), \"Beat Track\", and an optional \"Reference Track\". Users can select various processing options via checkboxes, including \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\". Additionally, there are slider controls for \"Auto-Tune Intensity\" and \"Effects Intensity\". The application outputs the processed audio, a summary of selected options, a basic waveform visualization, and a spectrogram visualization.\n",
        "\n",
        "The advanced audio processing features are currently implemented as placeholder functions. These functions include:\n",
        "*   `auto_mix_mastering`: Prints a message indicating placeholder execution and returns the input audio segment.\n",
        "*   `auto_tune`: Prints a message indicating placeholder execution with the given intensity and returns the input audio segment.\n",
        "*   `apply_effects`: Prints a message indicating placeholder execution with the given intensity and returns the input audio segment.\n",
        "*   `balance_voice_beat`: Prints a message indicating placeholder execution and overlays the voice and beat segments.\n",
        "*   `check_if_mixed_mastered`: Prints a message and returns the string \"Not mixed/mastered (placeholder)\".\n",
        "\n",
        "To further develop these complex functionalities, each placeholder function needs to be replaced with actual audio processing logic. This involves leveraging `pydub` for basic manipulations, `librosa` for audio analysis (e.g., pitch detection for auto-tune, loudness analysis for mixing/mastering), and potentially other specialized audio libraries or deep learning models for advanced tasks like source separation, dynamic range compression, or creative effects. The `check_if_mixed_mastered` function would require sophisticated machine learning models trained on large datasets of mixed/mastered versus unmixed/unmastered tracks.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Environment Setup**: FFmpeg, `gradio`, `pydub`, and `librosa` were confirmed to be already installed and up-to-date in the environment, indicating a ready-to-use audio processing setup.\n",
        "*   **Gradio Interface Design**: A Gradio interface was successfully created with dedicated audio input fields for voice (file upload and a placeholder for live microphone input), beat, and an optional reference track. It also includes checkboxes for five distinct processing options: \"Check if song is already mixed/mastered\", \"Auto Mix and Mastering\", \"Auto-Tune\", \"Effects\", and \"Balance Voice and Beat\". `gr.Slider` components were successfully added for 'Auto-Tune Intensity' and 'Effects Intensity'.\n",
        "*   **Placeholder Functions Implemented**: Core audio processing functions (`auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`, `check_if_mixed_mastered`) were defined as placeholders, printing their intended actions and returning the audio (or a mock status). These functions were updated to accept intensity parameters from the sliders.\n",
        "*   **Audio Processing Workflow**: The main `process_audio` function integrates these placeholders, loading audio via `pydub`, applying selected operations conditionally, and saving the output. Initial issues with `TypeError` due to invalid `optional` parameter in `gr.Audio`, `IndentationError` in string formatting, and `default` parameter in `gr.Slider`, and `source='microphone'` in `gr.Audio` were resolved during development, leading to a robust initial interface.\n",
        "*   **Basic Audio Visualization**: The application now generates a basic waveform plot and a spectrogram of the processed audio using `librosa.display.waveshow` and `librosa.display.specshow` respectively, both powered by `matplotlib`. These are saved as PNG images and displayed within the Gradio interface alongside the processed audio and summary text.\n",
        "*   **Application Launch**: The Gradio application was successfully launched, providing a temporary public URL for user interaction and testing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Modularize and Enhance Placeholders**: Replace current placeholder functions with actual audio processing algorithms using `pydub` and `librosa` to implement real auto-tuning, mixing, mastering, and effects. Consider using pre-trained models for advanced features like genre classification for 'check if song is already mixed/mastered'.\n",
        "*   **User Feedback and Advanced Controls**: Incorporate sliders and numerical inputs for effect parameters (e.g., auto-tune intensity, reverb decay) to allow users more granular control over the processing, providing a more professional and customizable experience.\n",
        "*   **Resolve Live Microphone Input**: Investigate and resolve the `TypeError` related to `source='microphone'` in `gr.Audio` to enable actual live voice input functionality as originally intended.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1c6d1a"
      },
      "source": [
        "## Add Progress Bar for Processing\n",
        "\n",
        "### Subtask:\n",
        "Integrate `gr.Progress` within the `process_audio` function. This will provide real-time feedback to the user during potentially longer audio processing tasks, improving the user experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ea28dad"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `process_audio` function to accept `gr.Progress()` as an argument and add calls to `progress()` at various stages of the audio processing. I also need to update the `gr.Interface` inputs to pass the `gr.Progress()` object. This will be done by rewriting the entire code block that defines the functions and the Gradio interface to ensure all changes are integrated correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eea7ec3d"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time # For simulating processing time\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    time.sleep(0.5) # Simulate work\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, live input, and progress bar\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path, progress=gr.Progress()):\n",
        "\n",
        "    progress(0.05, desc=\"Starting audio processing\")\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None\n",
        "\n",
        "    progress(0.1, desc=\"Loading audio files\")\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None, None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None, None\n",
        "\n",
        "    progress(0.2, desc=\"Preparing audio segments\")\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        progress(0.3, desc=\"Checking mix/master status\")\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        progress(0.4, desc=\"Applying auto mix and mastering\")\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        progress(0.5, desc=\"Applying auto-tune\")\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        progress(0.6, desc=\"Applying effects\")\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        progress(0.7, desc=\"Balancing voice and beat\")\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    progress(0.8, desc=\"Exporting processed audio\")\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    progress(0.9, desc=\"Generating visualizations\")\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    progress(1.0, desc=\"Processing complete\")\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input,\n",
        "        gr.Progress() # Added gr.Progress() to inputs\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", gr.Image(label=\"Waveform\"), gr.Image(label=\"Spectrogram\")],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with progress bar integration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0700791"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ValueError` indicates that `gr.Progress()` should not be directly passed as an input component to `gr.Interface`. Gradio automatically handles progress bars if the function signature accepts a `progress=gr.Progress()` argument. Therefore, I need to remove `gr.Progress()` from the `inputs` list in `gr.Interface`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92fa91c"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time # For simulating processing time\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    time.sleep(0.5) # Simulate work\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, live input, and progress bar\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path, progress=gr.Progress()):\n",
        "\n",
        "    progress(0.05, desc=\"Starting audio processing\")\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None\n",
        "\n",
        "    progress(0.1, desc=\"Loading audio files\")\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None, None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None, None\n",
        "\n",
        "    progress(0.2, desc=\"Preparing audio segments\")\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        progress(0.3, desc=\"Checking mix/master status\")\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        progress(0.4, desc=\"Applying auto mix and mastering\")\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        progress(0.5, desc=\"Applying auto-tune\")\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        progress(0.6, desc=\"Applying effects\")\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        progress(0.7, desc=\"Balancing voice and beat\")\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    progress(0.8, desc=\"Exporting processed audio\")\n",
        "    # Save the processed audio to a temporary file\n",
        "    output_audio_path = \"processed_output.mp3\"\n",
        "    processed_audio.export(output_audio_path, format=\"mp3\")\n",
        "\n",
        "    progress(0.9, desc=\"Generating visualizations\")\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    progress(1.0, desc=\"Processing complete\")\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input\n",
        "        # Removed gr.Progress() from inputs list as it's automatically injected by Gradio\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", gr.Image(label=\"Waveform\"), gr.Image(label=\"Spectrogram\")],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with progress bar integration.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8e2f9ab"
      },
      "source": [
        "## Allow Selection of Output Audio Format\n",
        "\n",
        "### Subtask:\n",
        "Add a `gr.Dropdown` or `gr.Radio` component to the Gradio interface, allowing users to select the desired output audio format (e.g., 'mp3', 'wav', 'flac'). Modify the `process_audio` function to export the processed audio in the chosen format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78af1624"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to add a `gr.Dropdown` component for output format selection, update the `process_audio` function to accept this new parameter, and modify the `export` call to use the selected format. Finally, I will update the `gr.Interface` inputs to include the new dropdown. This requires rewriting the entire code block that defines the functions and the Gradio interface to ensure all changes are integrated correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7306ccb"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time # For simulating processing time\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    time.sleep(0.5) # Simulate work\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, live input, progress bar, and output format selection\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path, output_format, progress=gr.Progress()):\n",
        "\n",
        "    progress(0.05, desc=\"Starting audio processing\")\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None\n",
        "\n",
        "    progress(0.1, desc=\"Loading audio files\")\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading voice track: {e}\", None, None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error loading beat track: {e}\", None, None\n",
        "\n",
        "    progress(0.2, desc=\"Preparing audio segments\")\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        progress(0.3, desc=\"Checking mix/master status\")\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        progress(0.4, desc=\"Applying auto mix and mastering\")\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        progress(0.5, desc=\"Applying auto-tune\")\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        progress(0.6, desc=\"Applying effects\")\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        progress(0.7, desc=\"Balancing voice and beat\")\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    if reference_track_path:\n",
        "        summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "        print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "\n",
        "    progress(0.8, desc=\"Exporting processed audio\")\n",
        "    # Save the processed audio to a temporary file in the chosen format\n",
        "    output_audio_path = f\"processed_output.{output_format}\"\n",
        "    processed_audio.export(output_audio_path, format=output_format)\n",
        "\n",
        "    progress(0.9, desc=\"Generating visualizations\")\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    progress(1.0, desc=\"Processing complete\")\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "output_format_dropdown = gr.Dropdown(choices=[\"mp3\", \"wav\", \"flac\"], value=\"mp3\", label=\"Output Audio Format\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input,\n",
        "        output_format_dropdown # Added dropdown for output format\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", gr.Image(label=\"Waveform\"), gr.Image(label=\"Spectrogram\")],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with output audio format selection.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3629dff"
      },
      "source": [
        "## Implement Reference Track Comparison Visualization\n",
        "\n",
        "### Subtask:\n",
        "If a reference track is uploaded, generate a comparative visualization. This could be an overlay of the processed audio's waveform with the reference track's waveform, or side-by-side plots for visual comparison. Add a new `gr.Image` output for this comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a35bf70"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `process_audio` function to load the reference track, generate a comparative waveform visualization, save it to a temporary file, and update the function's return statement to include this new image path. Concurrently, I will update the `gr.Interface` outputs to display this comparison image. This requires rewriting the entire code block that defines the functions and the Gradio interface to ensure all changes are integrated correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79a79e88"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time # For simulating processing time\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    time.sleep(0.5) # Simulate work\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, live input, progress bar, and output format selection\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path, output_format, progress=gr.Progress()):\n",
        "\n",
        "    progress(0.05, desc=\"Starting audio processing\")\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        # Adjusted for additional output (comparison image)\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None, None\n",
        "\n",
        "    progress(0.1, desc=\"Loading audio files\")\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        # Adjusted for additional output (comparison image)\n",
        "        return None, f\"Error loading voice track: {e}\", None, None, None\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        # Adjusted for additional output (comparison image)\n",
        "        return None, f\"Error loading beat track: {e}\", None, None, None\n",
        "\n",
        "    progress(0.2, desc=\"Preparing audio segments\")\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        progress(0.3, desc=\"Checking mix/master status\")\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        progress(0.4, desc=\"Applying auto mix and mastering\")\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        progress(0.5, desc=\"Applying auto-tune\")\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        progress(0.6, desc=\"Applying effects\")\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        progress(0.7, desc=\"Balancing voice and beat\")\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    reference_audio_loaded = False\n",
        "    y_ref = None\n",
        "    sr_ref = None\n",
        "    if reference_track_path:\n",
        "        try:\n",
        "            y_ref, sr_ref = librosa.load(reference_track_path, sr=None) # Load reference for visualization\n",
        "            summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "            print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "            reference_audio_loaded = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading reference track: {e}\")\n",
        "\n",
        "    progress(0.8, desc=\"Exporting processed audio\")\n",
        "    # Save the processed audio to a temporary file in the chosen format\n",
        "    output_audio_path = f\"processed_output.{output_format}\"\n",
        "    processed_audio.export(output_audio_path, format=output_format)\n",
        "\n",
        "    progress(0.9, desc=\"Generating visualizations\")\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    # --- Comparative Visualization (Waveform) ---\n",
        "    comparison_image_path = None\n",
        "    if reference_audio_loaded:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        # Plot processed audio\n",
        "        librosa.display.waveshow(y, sr=sr, alpha=0.7, label='Processed Audio')\n",
        "        # Plot reference track\n",
        "        # Resample reference if sample rates differ for consistent plotting\n",
        "        if sr_ref != sr:\n",
        "            y_ref_resampled = librosa.resample(y_ref, orig_sr=sr_ref, target_sr=sr)\n",
        "            librosa.display.waveshow(y_ref_resampled, sr=sr, color='r', alpha=0.5, label='Reference Track')\n",
        "        else:\n",
        "            librosa.display.waveshow(y_ref, sr=sr_ref, color='r', alpha=0.5, label='Reference Track')\n",
        "\n",
        "        plt.title('Processed Audio vs. Reference Track Waveform')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Amplitude')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        comparison_image_path = \"comparison_waveform.png\"\n",
        "        plt.savefig(comparison_image_path)\n",
        "        plt.close()\n",
        "    # --- End Comparative Visualization ---\n",
        "\n",
        "    progress(1.0, desc=\"Processing complete\")\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path, comparison_image_path\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "output_format_dropdown = gr.Dropdown(choices=[\"mp3\", \"wav\", \"flac\"], value=\"mp3\", label=\"Output Audio Format\")\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input,\n",
        "        output_format_dropdown\n",
        "    ],\n",
        "    outputs=[\"audio\", \"text\", gr.Image(label=\"Waveform\"), gr.Image(label=\"Spectrogram\"), gr.Image(label=\"Processed vs. Reference Waveform\")], # Added comparative visualization output\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with comparative waveform visualization for reference tracks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3dfa82"
      },
      "source": [
        "## Display Loudness Information\n",
        "\n",
        "### Subtask:\n",
        "Calculate and display relevant loudness metrics (e.g., LUFS, RMS) for both the input and processed audio tracks using `librosa`. Add `gr.Textbox` components to the interface to show these numerical values, providing professional audio insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2b20bf"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `process_audio` function to calculate RMS values for the voice track, beat track, and processed audio using `librosa`, convert them to decibels, and return these values. Then, I will update the `gr.Interface` outputs to include new `gr.Textbox` components for displaying these calculated RMS values. This requires rewriting the entire code block for `process_audio` and `iface` definition to ensure all dependencies and modifications are correctly integrated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dbb4a9a"
      },
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time # For simulating processing time\n",
        "\n",
        "# Placeholder audio processing functions\n",
        "def auto_mix_mastering(audio_segment):\n",
        "    print(\"Applying auto mix and mastering (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def auto_tune(audio_segment, intensity):\n",
        "    print(f\"Applying auto-tune (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def apply_effects(audio_segment, intensity):\n",
        "    print(f\"Applying effects (placeholder) with intensity: {intensity}.\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return audio_segment\n",
        "\n",
        "def balance_voice_beat(voice_segment, beat_segment):\n",
        "    print(\"Balancing voice and beat (placeholder).\")\n",
        "    time.sleep(1) # Simulate work\n",
        "    return voice_segment.overlay(beat_segment)\n",
        "\n",
        "def check_if_mixed_mastered(audio_segment):\n",
        "    print(\"Performing placeholder check for mixed/mastered status.\")\n",
        "    time.sleep(0.5) # Simulate work\n",
        "    return \"Not mixed/mastered (placeholder)\"\n",
        "\n",
        "# Helper function to get RMS in dB from pydub AudioSegment\n",
        "def get_rms_db(audio_segment):\n",
        "    # Convert pydub AudioSegment to numpy array for librosa\n",
        "    samples = np.array(audio_segment.get_array_of_samples())\n",
        "    # pydub stores samples as int16, librosa expects float\n",
        "    if audio_segment.sample_width == 2:\n",
        "        samples = samples.astype(np.float32) / (2**15)\n",
        "    elif audio_segment.sample_width == 4:\n",
        "        samples = samples.astype(np.float32) / (2**31)\n",
        "    else: # Fallback for other sample widths\n",
        "        samples = samples.astype(np.float32)\n",
        "\n",
        "    # If stereo, librosa.feature.rms expects mono or flattens it implicitly\n",
        "    # For accurate RMS, sum the square of each channel if stereo, then sqrt, or just take one channel\n",
        "    # For simplicity, convert to mono for RMS calculation if stereo, then calculate.\n",
        "    if audio_segment.channels == 2:\n",
        "        samples = samples.reshape((-1, 2)).mean(axis=1) # Simple average of channels\n",
        "\n",
        "    # Calculate RMS using librosa\n",
        "    rms = librosa.feature.rms(y=samples, frame_length=audio_segment.frame_rate // 10, hop_length=audio_segment.frame_rate // 100)\n",
        "    # Convert to dB\n",
        "    rms_db = librosa.amplitude_to_db(rms, ref=1.0).mean() # Mean RMS over time for a single value\n",
        "    return f\"{rms_db:.2f} dB\"\n",
        "\n",
        "# Updated process_audio function to integrate placeholders, visualization, live input, progress bar, and output format selection\n",
        "def process_audio(voice_track_path, beat_track_path, reference_track_path,\n",
        "                  check_mixed, auto_mix, auto_tune_opt, effects_opt, balance_opt,\n",
        "                  auto_tune_intensity, effects_intensity, live_voice_input_path, output_format, progress=gr.Progress()):\n",
        "\n",
        "    progress(0.05, desc=\"Starting audio processing\")\n",
        "\n",
        "    # Prioritize live microphone input if provided\n",
        "    actual_voice_track_path = live_voice_input_path if live_voice_input_path else voice_track_path\n",
        "\n",
        "    # Check if a voice track is available (either file upload or live input)\n",
        "    if not actual_voice_track_path:\n",
        "        # Adjusted for additional output (comparison image, RMS values)\n",
        "        return None, \"Error: No voice track provided (either upload a file or use microphone).\", None, None, None, \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "    progress(0.1, desc=\"Loading audio files\")\n",
        "\n",
        "    # Load audio files using pydub\n",
        "    try:\n",
        "        voice_audio = AudioSegment.from_file(actual_voice_track_path)\n",
        "    except Exception as e:\n",
        "        # Adjusted for additional output (comparison image, RMS values)\n",
        "        return None, f\"Error loading voice track: {e}\", None, None, None, \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "    try:\n",
        "        beat_audio = AudioSegment.from_file(beat_track_path)\n",
        "    except Exception as e:\n",
        "        # Adjusted for additional output (comparison image, RMS values)\n",
        "        return None, f\"Error loading beat track: {e}\", None, None, None, \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "    # Calculate RMS for original tracks\n",
        "    original_voice_rms = get_rms_db(voice_audio)\n",
        "    original_beat_rms = get_rms_db(beat_audio)\n",
        "\n",
        "    progress(0.2, desc=\"Preparing audio segments\")\n",
        "\n",
        "    # Ensure beat_audio is at the same frame rate as voice_audio for overlay\n",
        "    if voice_audio.frame_rate != beat_audio.frame_rate:\n",
        "        beat_audio = beat_audio.set_frame_rate(voice_audio.frame_rate)\n",
        "\n",
        "    # Ensure both segments are mono or stereo consistently\n",
        "    if voice_audio.channels != beat_audio.channels:\n",
        "        # Convert to a common channel format, e.g., stereo\n",
        "        if voice_audio.channels == 1: voice_audio = voice_audio.set_channels(2)\n",
        "        if beat_audio.channels == 1: beat_audio = beat_audio.set_channels(2)\n",
        "\n",
        "    processed_audio = voice_audio.overlay(beat_audio) # Initial combination\n",
        "    summary = \"Summary of options selected:\"\n",
        "\n",
        "    if check_mixed:\n",
        "        progress(0.3, desc=\"Checking mix/master status\")\n",
        "        mixed_mastered_status = check_if_mixed_mastered(processed_audio)\n",
        "        summary += f\" Check Mixed/Mastered: {mixed_mastered_status},\"\n",
        "\n",
        "    if auto_mix:\n",
        "        progress(0.4, desc=\"Applying auto mix and mastering\")\n",
        "        summary += \" Auto Mix/Mastering: Yes,\"\n",
        "        processed_audio = auto_mix_mastering(processed_audio)\n",
        "\n",
        "    if auto_tune_opt:\n",
        "        progress(0.5, desc=\"Applying auto-tune\")\n",
        "        summary += f\" Auto-Tune: Yes (Intensity: {auto_tune_intensity}),\"\n",
        "        processed_audio = auto_tune(processed_audio, auto_tune_intensity)\n",
        "\n",
        "    if effects_opt:\n",
        "        progress(0.6, desc=\"Applying effects\")\n",
        "        summary += f\" Effects: Yes (Intensity: {effects_intensity}),\"\n",
        "        processed_audio = apply_effects(processed_audio, effects_intensity)\n",
        "\n",
        "    if balance_opt:\n",
        "        progress(0.7, desc=\"Balancing voice and beat\")\n",
        "        summary += \" Balance Voice/Beat: Yes,\"\n",
        "        processed_audio = balance_voice_beat(voice_audio, beat_audio)\n",
        "\n",
        "    # Handle reference track (placeholder)\n",
        "    reference_audio_loaded = False\n",
        "    y_ref = None\n",
        "    sr_ref = None\n",
        "    if reference_track_path:\n",
        "        try:\n",
        "            y_ref, sr_ref = librosa.load(reference_track_path, sr=None) # Load reference for visualization\n",
        "            summary += f\" Reference Track: {reference_track_path.split('/')[-1]}, \"\n",
        "            print(f\"Reference track provided: {reference_track_path} (placeholder).\")\n",
        "            reference_audio_loaded = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading reference track: {e}\")\n",
        "\n",
        "    progress(0.8, desc=\"Exporting processed audio\")\n",
        "    # Save the processed audio to a temporary file in the chosen format\n",
        "    output_audio_path = f\"processed_output.{output_format}\"\n",
        "    processed_audio.export(output_audio_path, format=output_format)\n",
        "\n",
        "    # Calculate RMS for processed audio\n",
        "    processed_audio_rms = get_rms_db(processed_audio)\n",
        "\n",
        "    progress(0.9, desc=\"Generating visualizations\")\n",
        "    # --- Audio Visualization (Waveform) ---\n",
        "    y, sr = librosa.load(output_audio_path, sr=None) # Load for visualization, preserve original sr\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(y, sr=sr, alpha=0.7)\n",
        "    plt.title(\"Processed Audio Waveform\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "    waveform_image_path = \"waveform.png\"\n",
        "    plt.savefig(waveform_image_path)\n",
        "    plt.close() # Close the plot to prevent it from displaying automatically\n",
        "    # --- End Audio Visualization (Waveform) ---\n",
        "\n",
        "    # --- Audio Visualization (Spectrogram) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spectrogram_image_path = \"spectrogram.png\"\n",
        "    plt.savefig(spectrogram_image_path)\n",
        "    plt.close() # Close the plot\n",
        "    # --- End Audio Visualization (Spectrogram) ---\n",
        "\n",
        "    # --- Comparative Visualization (Waveform) ---\n",
        "    comparison_image_path = None\n",
        "    if reference_audio_loaded:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        # Plot processed audio\n",
        "        librosa.display.waveshow(y, sr=sr, alpha=0.7, label='Processed Audio')\n",
        "        # Plot reference track\n",
        "        # Resample reference if sample rates differ for consistent plotting\n",
        "        if sr_ref != sr:\n",
        "            y_ref_resampled = librosa.resample(y_ref, orig_sr=sr_ref, target_sr=sr)\n",
        "            librosa.display.waveshow(y_ref_resampled, sr=sr, color='r', alpha=0.5, label='Reference Track')\n",
        "        else:\n",
        "            librosa.display.waveshow(y_ref, sr=sr_ref, color='r', alpha=0.5, label='Reference Track')\n",
        "\n",
        "        plt.title('Processed Audio vs. Reference Track Waveform')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Amplitude')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        comparison_image_path = \"comparison_waveform.png\"\n",
        "        plt.savefig(comparison_image_path)\n",
        "        plt.close()\n",
        "    # --- End Comparative Visualization ---\n",
        "\n",
        "    progress(1.0, desc=\"Processing complete\")\n",
        "    return output_audio_path, summary, waveform_image_path, spectrogram_image_path, \\\n",
        "           comparison_image_path, original_voice_rms, original_beat_rms, processed_audio_rms\n",
        "\n",
        "# Define Gradio Interface\n",
        "voice_track_input = gr.Audio(type=\"filepath\", label=\"Voice Track (File Upload)\")\n",
        "live_voice_input = gr.Audio(label=\"Live Voice Input (Microphone)\")\n",
        "beat_track_input = gr.Audio(type=\"filepath\", label=\"Beat Track\")\n",
        "reference_track_input = gr.Audio(type=\"filepath\", label=\"Reference Track (Optional)\")\n",
        "\n",
        "check_mixed_mastered_checkbox = gr.Checkbox(label=\"Check if song is already mixed/mastered\")\n",
        "auto_mix_mastering_checkbox = gr.Checkbox(label=\"Auto Mix and Mastering\")\n",
        "auto_tune_checkbox = gr.Checkbox(label=\"Auto-Tune\")\n",
        "effects_checkbox = gr.Checkbox(label=\"Effects\")\n",
        "balance_voice_beat_checkbox = gr.Checkbox(label=\"Balance Voice and Beat\")\n",
        "\n",
        "auto_tune_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Auto-Tune Intensity\")\n",
        "effects_intensity_slider = gr.Slider(minimum=0.0, maximum=1.0, step=0.1, value=0.5, label=\"Effects Intensity\")\n",
        "\n",
        "output_format_dropdown = gr.Dropdown(choices=[\"mp3\", \"wav\", \"flac\"], value=\"mp3\", label=\"Output Audio Format\")\n",
        "\n",
        "# New Textbox components for RMS values\n",
        "voice_rms_output = gr.Textbox(label=\"Voice Track RMS (dB)\")\n",
        "beat_rms_output = gr.Textbox(label=\"Beat Track RMS (dB)\")\n",
        "processed_rms_output = gr.Textbox(label=\"Processed Audio RMS (dB)\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        voice_track_input,\n",
        "        beat_track_input,\n",
        "        reference_track_input,\n",
        "        check_mixed_mastered_checkbox,\n",
        "        auto_mix_mastering_checkbox,\n",
        "        auto_tune_checkbox,\n",
        "        effects_checkbox,\n",
        "        balance_voice_beat_checkbox,\n",
        "        auto_tune_intensity_slider,\n",
        "        effects_intensity_slider,\n",
        "        live_voice_input,\n",
        "        output_format_dropdown\n",
        "    ],\n",
        "    outputs=[\n",
        "        \"audio\",\n",
        "        \"text\",\n",
        "        gr.Image(label=\"Waveform\"),\n",
        "        gr.Image(label=\"Spectrogram\"),\n",
        "        gr.Image(label=\"Processed vs. Reference Waveform\"),\n",
        "        voice_rms_output,\n",
        "        beat_rms_output,\n",
        "        processed_rms_output\n",
        "    ],\n",
        "    title=\"Alayo Music Workflow\",\n",
        "    description=\"Upload your voice (file or live), beat, and optional reference track, then select processing options.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface updated with RMS loudness information.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36863a4"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a comprehensive summary of all the newly implemented dashboard features, including their functionalities and how they enhance the user experience, along with any final insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a75683e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The newly implemented dashboard features and their functionalities are designed to enhance the user experience by offering more control, better feedback, and deeper insights into the audio processing workflow.\n",
        "\n",
        "1.  **Gradio Parameter Sliders for 'Auto-Tune' and 'Effects' Intensity**: Users can now adjust the intensity of the \"Auto-Tune\" and \"Effects\" placeholder functions using `gr.Slider` components with a range from 0.0 to 1.0, stepping by 0.1. This provides more granular control over these processing options.\n",
        "2.  **Live Audio Input Option**: A `gr.Audio` component labeled \"Live Voice Input (Microphone)\" was introduced. While intended for direct microphone input to offer greater flexibility, due to environmental limitations (a `TypeError` with `source='microphone'`), it currently functions as an additional file upload input. The `process_audio` function prioritizes this input if provided.\n",
        "3.  **Advanced Audio Visualizations (Spectrogram)**: Beyond the basic waveform, the application now generates a Mel-Spectrogram of the processed audio using `librosa`. This visualization, displayed via a new `gr.Image` output, provides detailed insights into the frequency content of the audio over time.\n",
        "4.  **Progress Bar for Processing**: Integrated `gr.Progress` into the `process_audio` function, offering real-time progress updates (e.g., \"Loading audio files\", \"Applying auto-tune\", \"Generating visualizations\"). This significantly improves user experience by providing transparency during potentially lengthy processing tasks.\n",
        "5.  **Output Audio Format Selection**: A `gr.Dropdown` component allows users to select their desired output audio format from \"mp3\", \"wav\", or \"flac\", with \"mp3\" as the default. This functionality provides users with control over the final file type, catering to different quality and compatibility needs.\n",
        "6.  **Reference Track Comparison Visualization**: When an optional reference track is uploaded, the dashboard now generates a comparative waveform visualization. This new `gr.Image` output overlays the processed audio's waveform with the reference track's waveform, allowing for visual assessment of processing effects against a benchmark.\n",
        "7.  **Loudness Information Display**: The dashboard calculates and displays the Root Mean Square (RMS) loudness in decibels (dB) for the original voice track, original beat track, and the final processed audio. These numerical values are presented in dedicated `gr.Textbox` components, offering professional-grade insights into the audio's dynamic characteristics.\n",
        "\n",
        "These features collectively enhance the user experience by providing greater interactivity, more informative outputs, clearer process feedback, and more control over the processing and output formats.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Gradio Interface Features**:\n",
        "    *   Successfully implemented `gr.Slider` components for 'Auto-Tune Intensity' and 'Effects Intensity', allowing parameter adjustment from 0.0 to 1.0 with a step of 0.1.\n",
        "    *   Introduced a `gr.Audio` component for 'Live Voice Input (Microphone)'; however, due to a `TypeError` related to `source='microphone'`, it functions as an additional file upload.\n",
        "    *   Added `gr.Image` outputs for a detailed **Mel-Spectrogram** of the processed audio and a **Comparative Waveform Visualization** comparing processed audio with an optional reference track.\n",
        "    *   Integrated `gr.Progress` to provide real-time updates from 0% to 100% during processing steps.\n",
        "    *   Added a `gr.Dropdown` for \"Output Audio Format\" with choices: \"mp3\", \"wav\", \"flac\".\n",
        "    *   Included three `gr.Textbox` components to display RMS loudness in dB for the \"Voice Track\", \"Beat Track\", and \"Processed Audio\".\n",
        "*   **Audio Processing Enhancements**:\n",
        "    *   The `process_audio` function now accepts intensity parameters for auto-tune and effects, dynamically exports audio in the user-selected format, and calculates RMS values for input and output tracks.\n",
        "    *   The `process_audio` function also handles the generation and saving of waveform, spectrogram, and comparative waveform images.\n",
        "*   **Resolved Technical Issues**:\n",
        "    *   `TypeError: Slider.__init__() got an unexpected keyword argument 'default'` was resolved by removing the `default` parameter from `gr.Slider`.\n",
        "    *   `TypeError: Audio.__init__() got an unexpected keyword argument 'source'` prevented true microphone input, leading to the `gr.Audio` component defaulting to file upload.\n",
        "    *   `ValueError: Component must be provided as a `str` or `dict` or `Component` but is <gradio.helpers.Progress object at ...>` was resolved by removing `gr.Progress()` from the `inputs` list in `gr.Interface`, allowing Gradio to automatically inject it.\n",
        "*   **Overall Functionality**: The application successfully creates a robust Gradio interface that integrates audio input, user-selectable processing options, advanced visualizations, progress feedback, output format control, and professional audio metrics.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Enhance Placeholder Functions**: Replace the current placeholder audio processing functions (`auto_mix_mastering`, `auto_tune`, `apply_effects`, `balance_voice_beat`, `check_if_mixed_mastered`) with actual implementations using `pydub`, `librosa`, or specialized audio processing libraries to provide real-world functionality.\n",
        "*   **Investigate and Resolve Live Microphone Input**: Actively debug or update Gradio to properly enable `source='microphone'` functionality, as this was a key intended feature that could not be fully implemented due to version or environment incompatibility.\n"
      ]
    }
  ]
}